{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<b>LING 193 - F22<br>\n",
        "This is the my implementation of the autocorrect system<br></b><br>\n",
        "\n",
        "Big shout out to my instructor - ***Andrew McInnerney***<br>\n",
        "He led me through the every linguistic concepts I learned in Ling 193.<br>\n",
        "**\"Download dictionary\"** and **\"1st Layer of Correction: Minimum edit distance\"** are modefied based on the code from Andrew."
      ],
      "metadata": {
        "id": "1TYb4TQx84Wx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooL_nQk2U1Q_"
      },
      "source": [
        "# Download dictionary (run needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKgNUpzzFROm",
        "outputId": "51bb50d7-d760-4c4e-b999-a06fdc780fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Got Class 7.zip\n",
            "-----------------------------\n",
            "\n",
            "*** Got stanford-corenlp-4.5.1.zip\n"
          ]
        }
      ],
      "source": [
        "# LOAD THE FILES FOR THIS NOTEBOOK\n",
        "import pathlib\n",
        "from zipfile import ZipFile\n",
        "\n",
        "current_path = pathlib.Path.cwd()\n",
        "file_path = current_path.joinpath(\"Class 7.zip\")\n",
        "# print(f\"check exists: {pathlib.Path.exists(file_path)}\")\n",
        "if not pathlib.Path.exists(file_path):\n",
        "\t!wget -q --show-progress --no-check-certificate 'https://docs.google.com/uc?export=download&id=1cuIbCCZZutqlO-d9s9KxI6GxfbHd8rnb' -O 'Class 7.zip'\n",
        "\twith ZipFile('Class 7.zip', 'r') as zipObj:\n",
        "\t\tzipObj.extractall()\n",
        "print(\"*** Got Class 7.zip\")\n",
        "print(\"-----------------------------\\n\")\n",
        "\n",
        "file_path = current_path.joinpath(\"stanford-corenlp-4.5.1.zip\")\n",
        "if not pathlib.Path.exists(file_path):\n",
        "\t!wget https://nlp.stanford.edu/software/stanford-corenlp-4.5.1.zip\n",
        "\twith ZipFile('stanford-corenlp-4.5.1.zip', 'r') as zipObj:\n",
        "\t\tzipObj.extractall()\n",
        "print(\"*** Got stanford-corenlp-4.5.1.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSwAF2zwU1RD"
      },
      "source": [
        "# 1st Layer of Correction: Minimum edit distance (run needed)\n",
        "**minedit()** returns a table with every word in the dictionary listed along with its distance to the target word\n",
        "\n",
        "\n",
        "> **firstrow()**, **nextrow()**, and **substitution_penalty()** are the helper functions associated with **minedit()** in calculating word distance\n",
        "\n",
        "\n",
        "**get_mega_candidate()** will find the top 100 candidates with the minimum edit distance to the typo\n",
        "\n",
        "\n",
        "> ### *How do I index each word*:\n",
        "> *   get_mega_candidate()   =>   [ (word, min_distance) ... () ... ]\n",
        "> *   get_mega_candidate()   =>   [ ('THIS', 1) ... () ... ]\n",
        "> *   get_mega_candidate()[0]   =>   ('THIS', 1)\n",
        "> *   get_mega_candidate()[0][0]   =>   'THIS'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAYfb4-TTZ99"
      },
      "outputs": [],
      "source": [
        "def firstrow(m):\n",
        "    return [i for i in range(m+1)]\n",
        "\n",
        "\n",
        "def substitution_penalty(letter1, letter2):\n",
        "    if letter1 == letter2:\n",
        "        return 0\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "\n",
        "def nextrow(priorrow, word1, letter):\n",
        "    row = [priorrow[0] + 1]\n",
        "    priorcell = row[0]\n",
        "    for i in range(len(word1)):\n",
        "        insertion = priorrow[i+1] + 1\n",
        "        deletion = priorcell + 1\n",
        "        substitution = priorrow[i] + substitution_penalty(word1[i], letter)\n",
        "        priorcell = min(insertion, deletion, substitution)\n",
        "        row.append(priorcell)\n",
        "    return row\n",
        "\n",
        "\n",
        "def minedit(word1, word2):\n",
        "    m = len(word1)\n",
        "    n = len(word2)\n",
        "    priorrow = firstrow(m)\n",
        "    table = [priorrow]\n",
        "    for i in range(n):\n",
        "        row = nextrow(priorrow, word1, word2[i])\n",
        "        table.append(row)\n",
        "        priorrow = row\n",
        "    return table\n",
        "\n",
        "\n",
        "# This changes our dictionary to a shorter one of more frequent words\n",
        "with open(\"subtlex_words.txt\") as file:\n",
        "    words = file.read().splitlines()\n",
        "\n",
        "frequencies = {}\n",
        "for entry in words:\n",
        "    word = entry.split()[0].upper()\n",
        "    freq = int(entry.split()[1])\n",
        "    if word not in frequencies:\n",
        "        frequencies[word] = freq\n",
        "dictionary = {word:freq for word,freq in frequencies.items() if freq > 12}\n",
        "\n",
        "\n",
        "def get_mega_candidate(typo):\n",
        "    distance = {}\n",
        "    candidate = []\n",
        "    for word in dictionary:\n",
        "        table = minedit(typo,word)\n",
        "        distance[word] = table[-1][-1]\n",
        "\n",
        "    for i in range(100):\n",
        "        temp_word = min(distance, key = distance.get)\n",
        "        temp_val = distance[temp_word]\n",
        "        candidate.append((temp_word, temp_val))\n",
        "        del distance[temp_word]\n",
        "\n",
        "    return candidate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the 1nd Layer of Autocorrect System: Minimum Edit Distance\n",
        "Namely, if get_mega_candidate returns a list of candidates"
      ],
      "metadata": {
        "id": "rij5Py6It7FQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkP5_xpDU1RF",
        "outputId": "7ab0d0b6-f577-4339-93a3-da58e912a2d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('AN', 1), ('NI', 1), ('AI', 1), ('ANTI', 1), ('A', 2), ('AND', 2), ('I', 2), ('CAN', 2), ('MAN', 2), ('ANY', 2), ('AIN', 2), ('AIR', 2), ('RAN', 2), ('FAN', 2), ('N', 2), ('SAN', 2), ('VAN', 2), ('PANIC', 2), ('AIM', 2), ('AID', 2), ('DAN', 2), ('PAN', 2), ('TAN', 2), ('ANN', 2), ('ANNIE', 2), ('NIP', 2), ('ANT', 2), ('LAN', 2), ('BAN', 2), ('HAN', 2), ('ANA', 2), ('WAN', 2), ('TAI', 2), ('MANIC', 2), ('NIX', 2), ('MANIA', 2), ('AMI', 2), ('NAN', 2), ('ANVIL', 2), ('YAN', 2), ('KAI', 2), ('LAI', 2), ('HANOI', 2), ('NIL', 2), ('UNI', 2), ('BAI', 2), ('TANIA', 2), ('SAI', 2), ('AHI', 2), ('JAI', 2), ('NIG', 2), ('CAI', 2), ('KAN', 2), ('NIM', 2), ('IT', 3), ('IN', 3), ('ON', 3), ('IS', 3), ('NO', 3), ('AT', 3), ('IF', 3), ('WANT', 3), ('AS', 3), ('MEAN', 3), ('SAID', 3), ('AM', 3), ('THAN', 3), ('WAIT', 3), ('NICE', 3), ('WASN', 3), ('MANY', 3), ('HI', 3), ('HAND', 3), ('AREN', 3), ('DAMN', 3), ('HANG', 3), ('HAIR', 3), ('PLAN', 3), ('AH', 3), ('HASN', 3), ('FAIR', 3), ('MA', 3), ('PAID', 3), ('PAIN', 3), ('YA', 3), ('LAND', 3), ('NINE', 3), ('JAIL', 3), ('BANK', 3), ('RAIN', 3), ('MAIN', 3), ('ANIMAL', 3), ('PAIR', 3), ('AW', 3), ('LAID', 3), ('BAND', 3), ('MAIL', 3), ('HA', 3), ('LA', 3), ('LN', 3)]\n"
          ]
        }
      ],
      "source": [
        "# test correct()\n",
        "typo = \"ani\".upper()\n",
        "print(get_mega_candidate(typo))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2nd Layer of Correcton: Word Frequency (run needed)\n",
        "**citformat()** is a helper function that will clean up the punctuation in the given sentence. Its input is each word in the given function. Its output is the cleanned word.\n",
        "\n",
        "**get_freq_candidate()** will find the correct candidate with the minimum edit distance to the typo. This candidate will also has its word frequency to be the highest among other candidates\n",
        "\n",
        "**min_distance_correct()** is a driver function that takes in a sentence and return a sentence as the result of 1st layer of correction"
      ],
      "metadata": {
        "id": "8GC1rZA5eCYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def citformat(word):\n",
        "    punctuation = [\"!\",\"?\",\",\",\".\",\"-\",\"~\",\":\",\";\", \"'\", '\"']\n",
        "    while word[-1] in punctuation:\n",
        "        word = word[:-1]\n",
        "    while word[0] in punctuation:\n",
        "        word = word[1:]\n",
        "    return word.upper()\n",
        "\n",
        "\n",
        "def get_freq_candidate(typo):\n",
        "    errors = {}\n",
        "    mindist = 100\n",
        "    for word in dictionary:\n",
        "        table = minedit(typo,word)\n",
        "        distance = table[-1][-1]\n",
        "\n",
        "        if distance < mindist:\n",
        "          errors = {}\n",
        "          mindist = distance\n",
        "        if distance <= mindist:\n",
        "          errors[word] = distance\n",
        "\n",
        "    return max(errors, key = dictionary.get)\n",
        "\n",
        "\n",
        "def min_distance_correct(sentence):\n",
        "    sentence = sentence.split()\n",
        "    \n",
        "    for i in range(len(sentence)):\n",
        "        sentence[i] = citformat(sentence[i])\n",
        "        sentence[i] = get_freq_candidate(sentence[i].upper()).lower()\n",
        "    \n",
        "    sentence = \" \".join(sentence).capitalize()\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "b5M_bb8heH5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the 2nd Layer of Autocorrect System: Word Frequency\n"
      ],
      "metadata": {
        "id": "LAS8KewptuBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Examples:\n",
        "> s1 = \"The manager hired ths employees\"\\\n",
        "> s2 = \"John will not buy anu tomatoes\"\n",
        "\n",
        "\n",
        "### Min Distance + Frequency Result:\n",
        "> s1 = \"The manager hired **this** employees\"\\\n",
        "> s2 = \"John will not buy **an** tomatoes\""
      ],
      "metadata": {
        "id": "Czmiaq2Bfuv7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSLe6tq_3DW1",
        "outputId": "92bc409b-2ce7-415f-c8e4-0aa6193f0ca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The manager hired this employees\n",
            "John will not buy an tomatoes\n"
          ]
        }
      ],
      "source": [
        "# Examples\n",
        "s1 = \"The manager hired ths employees.\"\n",
        "s2 = \"John will not buy anu tomatoes.\"\n",
        "\n",
        "s1 = min_distance_correct(s1)\n",
        "print(s1)\n",
        "\n",
        "s2 = min_distance_correct(s2)\n",
        "print(s2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing for the 3rd Layer of Correction (run needed)"
      ],
      "metadata": {
        "id": "8Jz4fEoPhRNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies: nltk package"
      ],
      "metadata": {
        "id": "Hvrit5Q4zFuZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq_YL3a-XDM4",
        "outputId": "67f54cec-b84d-43ee-960a-64b19c3d0c4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Common \"DT\" from Brown Corpus"
      ],
      "metadata": {
        "id": "yKEIMW_WfE-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check each sentences in brwon corpus and find determiners showed in each sentences. \n",
        " \n",
        "Generate 3 lists: dt, dts, dti\n",
        "*   dt: singular determiner\n",
        "*   dts: plural determiner\n",
        "*   dti: plural quantifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0Dl079jJuJ9m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOPBS4h_HQpo",
        "outputId": "3217ce81-9104-4731-ec4b-8c13b39d883e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n",
            "dt: ['THIS', 'EACH', 'ANOTHER', 'THAT', \"'NOTHER\", 'THET']\n",
            "dts: ['THESE', 'THOSE', 'THEM', 'THEASE', 'THEES']\n",
            "dti: ['ANY', 'SOME', 'ENNY', 'ANI', 'ANYE']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "print(\"-----------------------------\")\n",
        "\n",
        "dt = []\n",
        "dts = []\n",
        "dti = []\n",
        "\n",
        "\n",
        "browntags = brown.tagged_sents()\n",
        "for sents in browntags:\n",
        "    for pair in sents:\n",
        "        determiner = pair[0].upper()\n",
        "        if pair[1] == \"DT\" and determiner not in dt:\n",
        "            dt.append(determiner)\n",
        "        elif pair[1] == \"DTS\" and determiner not in dts:\n",
        "            dts.append(determiner)\n",
        "        elif pair[1] == \"DTI\" and determiner not in dti:\n",
        "            dti.append(determiner)\n",
        "\n",
        "print(f\"dt: {dt}\")\n",
        "print(f\"dts: {dts}\")\n",
        "print(f\"dti: {dti}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean the Common \"DT\" from Brown Corpus"
      ],
      "metadata": {
        "id": "mjB8nnVWkI-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we narrow our expection for this autocorrect system, we can combine \"dt\" and \"dti\" into a single list. In other words, the singular determiner and singular/plural quantifier can be used interchangeably for many scenarios.\n",
        "\n",
        "This design would work out with our examples, sentence s1 & sentence s2. However, it also indicates a direction for further improvement for this autocorrect system. "
      ],
      "metadata": {
        "id": "YmIlUcG9qa0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean dt\n",
        "# Remove informal/colloquial singular determiners\n",
        "try:\n",
        "    dt.remove(\"'NOTHER\")\n",
        "    dt.remove(\"THET\")\n",
        "    dt.append(\"THE\")\n",
        "except:\n",
        "    print(\"dt is alreay cleanned\")\n",
        "\n",
        "dt.sort(key=dictionary.get, reverse=True) # Sort dt with word frequency\n",
        "print(f\"cleanned dt: {dt}\\n\")\n",
        "\n",
        "# Clean dts\n",
        "# Remove informal/colloquial plural determiners\n",
        "try:\n",
        "    dts.remove(\"THEASE\")\n",
        "    dts.remove(\"THEES\")\n",
        "    dts.remove(\"THEM\")\n",
        "    dts.append(\"THE\")\n",
        "except:\n",
        "    print(\"dts is alreay cleanned\")\n",
        "\n",
        "dts.sort(key=dictionary.get, reverse=True) # Sort dts with word frequency\n",
        "print(f\"cleanned dts: {dts}\\n\")\n",
        "\n",
        "# Clean dti\n",
        "# Remove informal/colloquial plural quantifier\n",
        "try:\n",
        "    dti.remove(\"ENNY\")\n",
        "    dti.remove(\"ANI\")\n",
        "    dti.remove(\"ANYE\")\n",
        "    dti.append(\"THE\")\n",
        "except:\n",
        "    print(\"dti is alreay cleanned\")\n",
        "\n",
        "dti.sort(key=dictionary.get, reverse=True) # Sort dti with word frequency\n",
        "print(f\"cleanned dti: {dti}\\n\")\n",
        "\n",
        "print(\"---------------------------------------------------\")\n",
        "# Merge dts and dti\n",
        "dt = dt + dti;\n",
        "dt = sorted(list(set(dt)), key=dictionary.get, reverse=True)\n",
        "print(f\"merged dt: {dt}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGVIhOpVkV6G",
        "outputId": "10f408be-610f-4f30-d70b-21b0df906060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cleanned dt: ['THE', 'THAT', 'THIS', 'ANOTHER', 'EACH']\n",
            "\n",
            "cleanned dts: ['THE', 'THESE', 'THOSE']\n",
            "\n",
            "cleanned dti: ['THE', 'SOME', 'ANY']\n",
            "\n",
            "---------------------------------------------------\n",
            "merged dt: ['THE', 'THAT', 'THIS', 'SOME', 'ANY', 'ANOTHER', 'EACH']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3rd Layer of Correction: Determiner & Noun Phrase (run needed)\n",
        "\n",
        "**analyze_tag()** is a helper function that takes in a sentence and returns a the tag for each words in the given sentence\n",
        "\n",
        "**find_determiner()** finds the correct determiner in the candidates and matches it with the determiners' set\n",
        "\n",
        "***determiner_correct()*** is the driver function for furter correction. It anlyzes each determiner and the noun phrase after it. If it finds a mismatch, it will change the form of the determiner to match the noun phrase."
      ],
      "metadata": {
        "id": "C_HUJOk61hlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing into Python\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def analyze_tag(sentence):\n",
        "    word_tag = nltk.pos_tag(word_tokenize(sentence))\n",
        "    return word_tag\n",
        "\n",
        "\n",
        "def find_determiner(candidates, determiner):\n",
        "\n",
        "    for idx in range(len(candidates)):\n",
        "        correct_word = candidates[idx][0]\n",
        "\n",
        "        if correct_word in determiner:\n",
        "            break\n",
        "\n",
        "    return correct_word\n",
        "\n",
        "\n",
        "def determiner_correct(word_tag):\n",
        "    result = []\n",
        "    for idx in range(len(word_tag)):\n",
        "        current_word = word_tag[idx][0]\n",
        "        current_tag = word_tag[idx][1]\n",
        "\n",
        "        if idx == len(word_tag) - 1:\n",
        "            # print(\"----------Last word----------\")\n",
        "            result.append(current_word)\n",
        "            # print(f\"add {current_word}\")\n",
        "            break\n",
        "\n",
        "        next_word = word_tag[idx + 1][0]\n",
        "        next_tag = word_tag[idx + 1][1]\n",
        "\n",
        "        candidates = get_mega_candidate(current_word.upper())\n",
        "        if current_tag == \"DT\" and next_tag == \"NNS\":\n",
        "            # print(\"----------NNS----------\")\n",
        "            # print(f\"NNS candidate: {candidates}\")\n",
        "\n",
        "            result.append(find_determiner(candidates, dts))\n",
        "            # print(f\"add {find_determiner(candidates, dts)}\")\n",
        "            \n",
        "        elif current_tag == \"DT\" and next_tag == \"NN\":\n",
        "            # print(\"----------NN----------\")\n",
        "            # print(f\"NN candidate: {candidates}\")\n",
        "\n",
        "            result.append(find_determiner(candidates, dt))\n",
        "            # print(f\"add {find_determiner(candidates, dt)}\")\n",
        "    \n",
        "        else:\n",
        "            # print(\"----------No Change----------\")\n",
        "            result.append(current_word)\n",
        "            # print(f\"add {current_word}\")\n",
        "\n",
        "    return \" \".join(result).capitalize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPTCP8EX1dw-",
        "outputId": "d4d6d3a5-c892-403a-d1d9-4eab2ef800f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Analyze Tags for the given examples, s1 & s2"
      ],
      "metadata": {
        "id": "x-2fF47AwHAz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-tUge9WXJF-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07ff54bc-8dde-4243-889a-d94543d21e96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('manager', 'NN'), ('hired', 'VBD'), ('this', 'DT'), ('employees', 'NNS')] \n",
            "\n",
            "[('John', 'NNP'), ('will', 'MD'), ('not', 'RB'), ('buy', 'VB'), ('an', 'DT'), ('tomatoes', 'NN')] \n",
            "\n",
            "[('ane', 'NN'), ('anu', 'NN'), ('anr', 'NN'), ('so', 'RB')] \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# Importing into Python\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Original sentences:\n",
        "# s1 = \"The manager hired ths employees.\"\n",
        "# s2 = \"John will not buy an tomatoes.\"\n",
        "\n",
        "# After the 1st layer of autocorrection:\n",
        "s1 = \"The manager hired this employees\"\n",
        "s2 = \"John will not buy an tomatoes\"\n",
        "test = \"ane anu anr so\"\n",
        "\n",
        "print(analyze_tag(s1), \"\\n\")\n",
        "print(analyze_tag(s2), \"\\n\")\n",
        "print(analyze_tag(test), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Driver function: Combine all function pieces (run needed)\n",
        "\n",
        "### Examples:\n",
        "> s1 = \"The manager hired ths employees\"\\\n",
        "> s2 = \"John will not buy anu tomatoes\"\n",
        "\n",
        "\n",
        "### Min Distance + Frequency Result:\n",
        "> s1 = \"The manager hired **this** employees\"\\\n",
        "> s2 = \"John will not buy **an** tomatoes\"\n",
        "\n",
        "### Determiner & Noun Phrase Result:\n",
        "> s1 = \"The manager hired **the** employees\"\\\n",
        "> s2 = \"John will not buy **any** tomatoes\""
      ],
      "metadata": {
        "id": "LjduS0gF_Nxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s1 = \"The manager hired thi employees.\"\n",
        "s2 = \"John will not buy anu tomatoes.\"\n",
        "\n",
        "def autocorrect(sentence):\n",
        "    print(f\"*** Original sentence: {sentence}\")\n",
        "\n",
        "    sentence = min_distance_correct(sentence).capitalize()\n",
        "    print(\"*** Min distance + Frequency result: \", sentence)\n",
        "\n",
        "    word_tag = analyze_tag(sentence)\n",
        "    # print(\"Tag:\", word_tag)\n",
        "\n",
        "    result = determiner_correct(word_tag)\n",
        "    # print(\"Final result: \", result, \"\\n\")\n",
        "\n",
        "    return result\n",
        "\n",
        "print(f\"*** Final result: {autocorrect(s1)}\\n\")\n",
        "print(f\"*** Final result: {autocorrect(s2)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wrvM3ce_bhL",
        "outputId": "82014742-91f5-4ac3-eb2a-be8f2f9d93f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Original sentence: The manager hired thi employees.\n",
            "*** Min distance + Frequency result:  The manager hired this employees\n",
            "*** Final result: The manager hired the employees\n",
            "\n",
            "*** Original sentence: John will not buy anu tomatoes.\n",
            "*** Min distance + Frequency result:  John will not buy an tomatoes\n",
            "*** Final result: John will not buy any tomatoes\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ooL_nQk2U1Q_",
        "XSwAF2zwU1RD",
        "rij5Py6It7FQ",
        "8GC1rZA5eCYw",
        "LAS8KewptuBB",
        "8Jz4fEoPhRNL",
        "yKEIMW_WfE-Y",
        "mjB8nnVWkI-T",
        "C_HUJOk61hlc"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}