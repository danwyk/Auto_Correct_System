{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TYb4TQx84Wx"
      },
      "source": [
        "<b>LING 193 - F22<br>\n",
        "This is the my implementation of the autocorrect system<br></b><br>\n",
        "\n",
        "Big shout out to my instructor - ***Andrew McInnerney***<br>\n",
        "He led me through every linguistic concepts I learned in Ling 193.<br>\n",
        "**\"Download dictionary\"** and **\"1st Layer of Correction: Minimum edit distance\"** are modefied based on the code from Andrew."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooL_nQk2U1Q_"
      },
      "source": [
        "# Download dictionary (run needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKgNUpzzFROm",
        "outputId": "523572ea-7911-409f-bce6-138cb2ac5732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rClass 7.zip           0%[                    ]       0  --.-KB/s               \rClass 7.zip         100%[===================>]   1.04M  --.-KB/s    in 0.008s  \n",
            "*** Got Class 7.zip\n",
            "-----------------------------\n",
            "\n",
            "--2022-12-30 07:41:55--  https://nlp.stanford.edu/software/stanford-corenlp-4.5.1.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-4.5.1.zip [following]\n",
            "--2022-12-30 07:41:56--  https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-4.5.1.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 505225173 (482M) [application/zip]\n",
            "Saving to: ‘stanford-corenlp-4.5.1.zip’\n",
            "\n",
            "stanford-corenlp-4. 100%[===================>] 481.82M  5.09MB/s    in 92s     \n",
            "\n",
            "2022-12-30 07:43:29 (5.24 MB/s) - ‘stanford-corenlp-4.5.1.zip’ saved [505225173/505225173]\n",
            "\n",
            "*** Got stanford-corenlp-4.5.1.zip\n"
          ]
        }
      ],
      "source": [
        "# LOAD THE FILES FOR THIS NOTEBOOK\n",
        "import pathlib\n",
        "from zipfile import ZipFile\n",
        "\n",
        "current_path = pathlib.Path.cwd()\n",
        "file_path = current_path.joinpath(\"Class 7.zip\")\n",
        "# print(f\"check exists: {pathlib.Path.exists(file_path)}\")\n",
        "if not pathlib.Path.exists(file_path):\n",
        "\t!wget -q --show-progress --no-check-certificate 'https://docs.google.com/uc?export=download&id=1cuIbCCZZutqlO-d9s9KxI6GxfbHd8rnb' -O 'Class 7.zip'\n",
        "\twith ZipFile('Class 7.zip', 'r') as zipObj:\n",
        "\t\tzipObj.extractall()\n",
        "print(\"*** Got Class 7.zip\")\n",
        "print(\"-----------------------------\\n\")\n",
        "\n",
        "file_path = current_path.joinpath(\"stanford-corenlp-4.5.1.zip\")\n",
        "if not pathlib.Path.exists(file_path):\n",
        "\t!wget https://nlp.stanford.edu/software/stanford-corenlp-4.5.1.zip\n",
        "\twith ZipFile('stanford-corenlp-4.5.1.zip', 'r') as zipObj:\n",
        "\t\tzipObj.extractall()\n",
        "print(\"*** Got stanford-corenlp-4.5.1.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSwAF2zwU1RD"
      },
      "source": [
        "# 1st Layer of Correction: Minimum edit distance (run needed)\n",
        "**minedit()** returns a table with every word in the dictionary listed along with its distance to the target word\n",
        "\n",
        "\n",
        "> **firstrow()**, **nextrow()**, and **substitution_penalty()** are the helper functions associated with **minedit()** in calculating word distance\n",
        "\n",
        "\n",
        "**get_mega_candidate()** will find the top 100 candidates with the minimum edit distance to the typo\n",
        "\n",
        "\n",
        "> ### *How do I index each word*:\n",
        "> *   get_mega_candidate()   =>   [ (word, min_distance) ... () ... ]\n",
        "> *   get_mega_candidate()   =>   [ ('THIS', 1) ... () ... ]\n",
        "> *   get_mega_candidate()[0]   =>   ('THIS', 1)\n",
        "> *   get_mega_candidate()[0][0]   =>   'THIS'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QAYfb4-TTZ99"
      },
      "outputs": [],
      "source": [
        "def firstrow(m):\n",
        "    return [i for i in range(m+1)]\n",
        "\n",
        "\n",
        "def substitution_penalty(letter1, letter2):\n",
        "    if letter1 == letter2:\n",
        "        return 0\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "\n",
        "def nextrow(priorrow, word1, letter):\n",
        "    row = [priorrow[0] + 1]\n",
        "    priorcell = row[0]\n",
        "    for i in range(len(word1)):\n",
        "        insertion = priorrow[i+1] + 1\n",
        "        deletion = priorcell + 1\n",
        "        substitution = priorrow[i] + substitution_penalty(word1[i], letter)\n",
        "        priorcell = min(insertion, deletion, substitution)\n",
        "        row.append(priorcell)\n",
        "    return row\n",
        "\n",
        "\n",
        "def minedit(word1, word2):\n",
        "    m = len(word1)\n",
        "    n = len(word2)\n",
        "    priorrow = firstrow(m)\n",
        "    table = [priorrow]\n",
        "    for i in range(n):\n",
        "        row = nextrow(priorrow, word1, word2[i])\n",
        "        table.append(row)\n",
        "        priorrow = row\n",
        "    return table\n",
        "\n",
        "\n",
        "# This changes our dictionary to a shorter one of more frequent words\n",
        "with open(\"subtlex_words.txt\") as file:\n",
        "    words = file.read().splitlines()\n",
        "\n",
        "frequencies = {}\n",
        "for entry in words:\n",
        "    word = entry.split()[0].upper()\n",
        "    freq = int(entry.split()[1])\n",
        "    if word not in frequencies:\n",
        "        frequencies[word] = freq\n",
        "dictionary = {word:freq for word,freq in frequencies.items() if freq > 12}\n",
        "\n",
        "\n",
        "def get_mega_candidate(typo):\n",
        "    distance = {}\n",
        "    candidate = []\n",
        "    for word in dictionary:\n",
        "        table = minedit(typo,word)\n",
        "        distance[word] = table[-1][-1]\n",
        "\n",
        "    for i in range(100):\n",
        "        temp_word = min(distance, key = distance.get)\n",
        "        temp_val = distance[temp_word]\n",
        "        candidate.append((temp_word, temp_val))\n",
        "        del distance[temp_word]\n",
        "\n",
        "    return candidate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rij5Py6It7FQ"
      },
      "source": [
        "## Test the 1nd Layer of Autocorrect System: Minimum Edit Distance\n",
        "Namely, if get_mega_candidate returns a list of candidates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkP5_xpDU1RF",
        "outputId": "2e40ea7f-c9e9-4d87-f247-abc2c5082494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('AN', 1), ('NI', 1), ('AI', 1), ('ANTI', 1), ('A', 2), ('AND', 2), ('I', 2), ('CAN', 2), ('MAN', 2), ('ANY', 2), ('AIN', 2), ('AIR', 2), ('RAN', 2), ('FAN', 2), ('N', 2), ('SAN', 2), ('VAN', 2), ('PANIC', 2), ('AIM', 2), ('AID', 2), ('DAN', 2), ('PAN', 2), ('TAN', 2), ('ANN', 2), ('ANNIE', 2), ('NIP', 2), ('ANT', 2), ('LAN', 2), ('BAN', 2), ('HAN', 2), ('ANA', 2), ('WAN', 2), ('TAI', 2), ('MANIC', 2), ('NIX', 2), ('MANIA', 2), ('AMI', 2), ('NAN', 2), ('ANVIL', 2), ('YAN', 2), ('KAI', 2), ('LAI', 2), ('HANOI', 2), ('NIL', 2), ('UNI', 2), ('BAI', 2), ('TANIA', 2), ('SAI', 2), ('AHI', 2), ('JAI', 2), ('NIG', 2), ('CAI', 2), ('KAN', 2), ('NIM', 2), ('IT', 3), ('IN', 3), ('ON', 3), ('IS', 3), ('NO', 3), ('AT', 3), ('IF', 3), ('WANT', 3), ('AS', 3), ('MEAN', 3), ('SAID', 3), ('AM', 3), ('THAN', 3), ('WAIT', 3), ('NICE', 3), ('WASN', 3), ('MANY', 3), ('HI', 3), ('HAND', 3), ('AREN', 3), ('DAMN', 3), ('HANG', 3), ('HAIR', 3), ('PLAN', 3), ('AH', 3), ('HASN', 3), ('FAIR', 3), ('MA', 3), ('PAID', 3), ('PAIN', 3), ('YA', 3), ('LAND', 3), ('NINE', 3), ('JAIL', 3), ('BANK', 3), ('RAIN', 3), ('MAIN', 3), ('ANIMAL', 3), ('PAIR', 3), ('AW', 3), ('LAID', 3), ('BAND', 3), ('MAIL', 3), ('HA', 3), ('LA', 3), ('LN', 3)]\n"
          ]
        }
      ],
      "source": [
        "# test correct()\n",
        "typo = \"ani\".upper()\n",
        "print(get_mega_candidate(typo))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GC1rZA5eCYw"
      },
      "source": [
        "# 2nd Layer of Correcton: Word Frequency (run needed)\n",
        "**citformat()** is a helper function that will clean up the punctuation in the given sentence. Its input is each word in the given function. Its output is the cleanned word.\n",
        "\n",
        "**get_freq_candidate()** will find the correct candidate with the minimum edit distance to the typo. This candidate will also has its word frequency to be the highest among other candidates\n",
        "\n",
        "**min_distance_correct()** is a driver function that takes in a sentence and return a sentence as the result of 1st layer of correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b5M_bb8heH5l"
      },
      "outputs": [],
      "source": [
        "def citformat(word):\n",
        "    punctuation = [\"!\",\"?\",\",\",\".\",\"-\",\"~\",\":\",\";\", \"'\", '\"']\n",
        "    while word[-1] in punctuation:\n",
        "        word = word[:-1]\n",
        "    while word[0] in punctuation:\n",
        "        word = word[1:]\n",
        "    return word.upper()\n",
        "\n",
        "\n",
        "def get_freq_candidate(typo):\n",
        "    errors = {}\n",
        "    mindist = 100\n",
        "    for word in dictionary:\n",
        "        table = minedit(typo,word)\n",
        "        distance = table[-1][-1]\n",
        "\n",
        "        if distance < mindist:\n",
        "          errors = {}\n",
        "          mindist = distance\n",
        "        if distance <= mindist:\n",
        "          errors[word] = distance\n",
        "\n",
        "    return max(errors, key = dictionary.get)\n",
        "\n",
        "\n",
        "def min_distance_correct(sentence):\n",
        "    sentence = sentence.split()\n",
        "    \n",
        "    for i in range(len(sentence)):\n",
        "        sentence[i] = citformat(sentence[i])\n",
        "        sentence[i] = get_freq_candidate(sentence[i].upper()).lower()\n",
        "    \n",
        "    sentence = \" \".join(sentence).capitalize()\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAS8KewptuBB"
      },
      "source": [
        "## Test the 2nd Layer of Autocorrect System: Word Frequency\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czmiaq2Bfuv7"
      },
      "source": [
        "### Examples:\n",
        "> s1 = \"The manager hired ths employees\"\\\n",
        "> s2 = \"John will not buy anu tomatoes\"\n",
        "\n",
        "\n",
        "### Min Distance + Frequency Result:\n",
        "> s1 = \"The manager hired **this** employees\"\\\n",
        "> s2 = \"John will not buy **an** tomatoes\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSLe6tq_3DW1",
        "outputId": "1c157899-5708-4a63-d0d9-c9b302ee8516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The manager hired this employees\n",
            "John will not buy an tomatoes\n"
          ]
        }
      ],
      "source": [
        "# Examples\n",
        "s1 = \"The manager hired ths employees.\"\n",
        "s2 = \"John will not buy anu tomatoes.\"\n",
        "\n",
        "s1 = min_distance_correct(s1)\n",
        "print(s1)\n",
        "\n",
        "s2 = min_distance_correct(s2)\n",
        "print(s2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Jz4fEoPhRNL"
      },
      "source": [
        "# Preparing for the 3rd Layer of Correction (run needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvrit5Q4zFuZ"
      },
      "source": [
        "Install dependencies: nltk package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq_YL3a-XDM4",
        "outputId": "56684730-0cb3-4fba-e024-3b2a52de188a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKEIMW_WfE-Y"
      },
      "source": [
        "## Find Common \"DT\" from Brown Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Dl079jJuJ9m"
      },
      "source": [
        "Check each sentences in brwon corpus and find determiners showed in each sentences. \n",
        " \n",
        "Generate 3 lists: dt, dts, dti\n",
        "*   dt: singular determiner\n",
        "*   dts: plural determiner\n",
        "*   dti: plural quantifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOPBS4h_HQpo",
        "outputId": "de445bc6-e173-4789-b0b0-f898d5e58201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n",
            "dt: ['THIS', 'EACH', 'ANOTHER', 'THAT', \"'NOTHER\", 'THET']\n",
            "dts: ['THESE', 'THOSE', 'THEM', 'THEASE', 'THEES']\n",
            "dti: ['ANY', 'SOME', 'ENNY', 'ANI', 'ANYE']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "print(\"-----------------------------\")\n",
        "\n",
        "dt = []\n",
        "dts = []\n",
        "dti = []\n",
        "\n",
        "\n",
        "browntags = brown.tagged_sents()\n",
        "for sents in browntags:\n",
        "    for pair in sents:\n",
        "        determiner = pair[0].upper()\n",
        "        if pair[1] == \"DT\" and determiner not in dt:\n",
        "            dt.append(determiner)\n",
        "        elif pair[1] == \"DTS\" and determiner not in dts:\n",
        "            dts.append(determiner)\n",
        "        elif pair[1] == \"DTI\" and determiner not in dti:\n",
        "            dti.append(determiner)\n",
        "\n",
        "print(f\"dt: {dt}\")\n",
        "print(f\"dts: {dts}\")\n",
        "print(f\"dti: {dti}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjB8nnVWkI-T"
      },
      "source": [
        "## Clean the Common \"DT\" from Brown Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmIlUcG9qa0Z"
      },
      "source": [
        "If we narrow our expection for this autocorrect system, we can combine \"dt\" and \"dti\" into a single list. In other words, the singular determiner and singular/plural quantifier can be used interchangeably for many scenarios.\n",
        "\n",
        "This design would work out with our examples, sentence s1 & sentence s2. However, it also indicates a direction for further improvement for this autocorrect system. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGVIhOpVkV6G",
        "outputId": "87142bd5-4a48-4c12-a8ac-a46187af9228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cleanned dt: ['THE', 'THAT', 'THIS', 'ANOTHER', 'EACH']\n",
            "\n",
            "cleanned dts: ['THE', 'THESE', 'THOSE']\n",
            "\n",
            "cleanned dti: ['THE', 'SOME', 'ANY']\n",
            "\n",
            "---------------------------------------------------\n",
            "merged dt: ['THE', 'THAT', 'THIS', 'SOME', 'ANY', 'ANOTHER', 'EACH']\n"
          ]
        }
      ],
      "source": [
        "# Clean dt\n",
        "# Remove informal/colloquial singular determiners\n",
        "try:\n",
        "    dt.remove(\"'NOTHER\")\n",
        "    dt.remove(\"THET\")\n",
        "    dt.append(\"THE\")\n",
        "except:\n",
        "    print(\"dt is alreay cleanned\")\n",
        "\n",
        "dt.sort(key=dictionary.get, reverse=True) # Sort dt with word frequency\n",
        "print(f\"cleanned dt: {dt}\\n\")\n",
        "\n",
        "# Clean dts\n",
        "# Remove informal/colloquial plural determiners\n",
        "try:\n",
        "    dts.remove(\"THEASE\")\n",
        "    dts.remove(\"THEES\")\n",
        "    dts.remove(\"THEM\")\n",
        "    dts.append(\"THE\")\n",
        "except:\n",
        "    print(\"dts is alreay cleanned\")\n",
        "\n",
        "dts.sort(key=dictionary.get, reverse=True) # Sort dts with word frequency\n",
        "print(f\"cleanned dts: {dts}\\n\")\n",
        "\n",
        "# Clean dti\n",
        "# Remove informal/colloquial plural quantifier\n",
        "try:\n",
        "    dti.remove(\"ENNY\")\n",
        "    dti.remove(\"ANI\")\n",
        "    dti.remove(\"ANYE\")\n",
        "    dti.append(\"THE\")\n",
        "except:\n",
        "    print(\"dti is alreay cleanned\")\n",
        "\n",
        "dti.sort(key=dictionary.get, reverse=True) # Sort dti with word frequency\n",
        "print(f\"cleanned dti: {dti}\\n\")\n",
        "\n",
        "print(\"---------------------------------------------------\")\n",
        "# Merge dts and dti\n",
        "dt = dt + dti;\n",
        "dt = sorted(list(set(dt)), key=dictionary.get, reverse=True)\n",
        "print(f\"merged dt: {dt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_HUJOk61hlc"
      },
      "source": [
        "# 3rd Layer of Correction: Determiner & Noun Phrase (run needed)\n",
        "\n",
        "**analyze_tag()** is a helper function that takes in a sentence and returns a the tag for each words in the given sentence\n",
        "\n",
        "**find_determiner()** finds the correct determiner in the candidates and matches it with the determiners' set\n",
        "\n",
        "***determiner_correct()*** is the driver function for furter correction. It anlyzes each determiner and the noun phrase after it. If it finds a mismatch, it will change the form of the determiner to match the noun phrase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPTCP8EX1dw-",
        "outputId": "10f9ca5d-964f-46e8-b2e9-85a8a0a0255f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# Importing into Python\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def analyze_tag(sentence):\n",
        "    word_tag = nltk.pos_tag(word_tokenize(sentence))\n",
        "    return word_tag\n",
        "\n",
        "\n",
        "def find_determiner(candidates, determiner):\n",
        "\n",
        "    for idx in range(len(candidates)):\n",
        "        correct_word = candidates[idx][0]\n",
        "\n",
        "        if correct_word in determiner:\n",
        "            break\n",
        "\n",
        "    return correct_word\n",
        "\n",
        "\n",
        "def determiner_correct(word_tag):\n",
        "    result = []\n",
        "    for idx in range(len(word_tag)):\n",
        "        current_word = word_tag[idx][0]\n",
        "        current_tag = word_tag[idx][1]\n",
        "\n",
        "        if idx == len(word_tag) - 1:\n",
        "            # print(\"----------Last word----------\")\n",
        "            result.append(current_word)\n",
        "            # print(f\"add {current_word}\")\n",
        "            break\n",
        "\n",
        "        next_word = word_tag[idx + 1][0]\n",
        "        next_tag = word_tag[idx + 1][1]\n",
        "\n",
        "        candidates = get_mega_candidate(current_word.upper())\n",
        "        if current_tag == \"DT\" and next_tag == \"NNS\":\n",
        "            # print(\"----------NNS----------\")\n",
        "            # print(f\"NNS candidate: {candidates}\")\n",
        "\n",
        "            result.append(find_determiner(candidates, dts))\n",
        "            # print(f\"add {find_determiner(candidates, dts)}\")\n",
        "            \n",
        "        elif current_tag == \"DT\" and next_tag == \"NN\":\n",
        "            # print(\"----------NN----------\")\n",
        "            # print(f\"NN candidate: {candidates}\")\n",
        "\n",
        "            result.append(find_determiner(candidates, dt))\n",
        "            # print(f\"add {find_determiner(candidates, dt)}\")\n",
        "    \n",
        "        else:\n",
        "            # print(\"----------No Change----------\")\n",
        "            result.append(current_word)\n",
        "            # print(f\"add {current_word}\")\n",
        "\n",
        "    return \" \".join(result).capitalize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-2fF47AwHAz"
      },
      "source": [
        "## Test Analyze Tags for the given examples, s1 & s2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-tUge9WXJF-",
        "outputId": "33aa6509-0876-4392-d6dd-fceab43522af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('manager', 'NN'), ('hired', 'VBD'), ('this', 'DT'), ('employees', 'NNS')] \n",
            "\n",
            "[('John', 'NNP'), ('will', 'MD'), ('not', 'RB'), ('buy', 'VB'), ('an', 'DT'), ('tomatoes', 'NN')] \n",
            "\n",
            "[('ane', 'NN'), ('anu', 'NN'), ('anr', 'NN'), ('so', 'RB')] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Importing into Python\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Original sentences:\n",
        "# s1 = \"The manager hired ths employees.\"\n",
        "# s2 = \"John will not buy an tomatoes.\"\n",
        "\n",
        "# After the 1st layer of autocorrection:\n",
        "s1 = \"The manager hired this employees\"\n",
        "s2 = \"John will not buy an tomatoes\"\n",
        "test = \"ane anu anr so\"\n",
        "\n",
        "print(analyze_tag(s1), \"\\n\")\n",
        "print(analyze_tag(s2), \"\\n\")\n",
        "print(analyze_tag(test), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjduS0gF_Nxv"
      },
      "source": [
        "# Driver function: Combine all function pieces (run needed)\n",
        "\n",
        "### Examples:\n",
        "> s1 = \"The manager hired ths employees\"\\\n",
        "> s2 = \"John will not buy anu tomatoes\"\n",
        "\n",
        "\n",
        "### Min Distance + Frequency Result:\n",
        "> s1 = \"The manager hired **this** employees\"\\\n",
        "> s2 = \"John will not buy **an** tomatoes\"\n",
        "\n",
        "### Determiner & Noun Phrase Result:\n",
        "> s1 = \"The manager hired **the** employees\"\\\n",
        "> s2 = \"John will not buy **any** tomatoes\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wrvM3ce_bhL",
        "outputId": "b337618a-72d7-4cd6-cd43-bd78efc98cb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Original sentence: The manager hired thi employees.\n",
            "*** Min distance + Frequency result:  The manager hired this employees\n",
            "*** Final result: The manager hired the employees\n",
            "\n",
            "*** Original sentence: John will not buy anu tomatoes.\n",
            "*** Min distance + Frequency result:  John will not buy an tomatoes\n",
            "*** Final result: John will not buy any tomatoes\n"
          ]
        }
      ],
      "source": [
        "s1 = \"The manager hired thi employees.\"\n",
        "s2 = \"John will not buy anu tomatoes.\"\n",
        "\n",
        "def autocorrect(sentence):\n",
        "    print(f\"*** Original sentence: {sentence}\")\n",
        "\n",
        "    sentence = min_distance_correct(sentence).capitalize()\n",
        "    print(\"*** Min distance + Frequency result: \", sentence)\n",
        "\n",
        "    word_tag = analyze_tag(sentence)\n",
        "    # print(\"Tag:\", word_tag)\n",
        "\n",
        "    result = determiner_correct(word_tag)\n",
        "    # print(\"Final result: \", result, \"\\n\")\n",
        "\n",
        "    return result\n",
        "\n",
        "print(f\"*** Final result: {autocorrect(s1)}\\n\")\n",
        "print(f\"*** Final result: {autocorrect(s2)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examples:\n",
        "### Sample:\n",
        "It is widelly taugt that parts of speech are definned in terms of sinple defimitions. For example, \"a noun is a person place or thinge.\" But in\n",
        "reality, simple defintions like that are not very usefful. For exampe, \n",
        "is it not true that everthing is a \"thing\"? Then in wghat sense can we\n",
        "deffine nouns as \"thngs\"? We need moree rigorouss teckneques to define\n",
        "these notioms.\n",
        "\n",
        "\n",
        "### Result:\n",
        "It is widely taught that parts of speech are defined in terms of simple definitions for example any noun is any person place or thing but in reality simple definitions like that are not very useful for example is it not true that everything is any thing then in what sense can we define nouns as things we need more rigorous techniques to define these notions"
      ],
      "metadata": {
        "id": "L1P1gpg8-x9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = \"\"\"It is widelly taugt that parts of speech are definned in terms of sinple\n",
        "defimitions. For example, \"a noun is a person place or thinge.\" But in\n",
        "reality, simple defintions like that are not very usefful. For exampe, \n",
        "is it not true that everthing is a \"thing\"? Then in wghat sense can we\n",
        "deffine nouns as \"thngs\"? We need moree rigorouss teckneques to define\n",
        "these notioms.\"\"\"\n",
        "\n",
        "\n",
        "print(f\"*** Final result: {autocorrect(sample)}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfvHWKWU-Q0X",
        "outputId": "717d61a4-01a5-4095-9fe4-998d1ec9f40a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Original sentence: It is widelly taugt that parts of speech are definned in terms of sinple\n",
            "defimitions. For example, \"a noun is a person place or thinge.\" But in\n",
            "reality, simple defintions like that are not very usefful. For exampe, \n",
            "is it not true that everthing is a \"thing\"? Then in wghat sense can we\n",
            "deffine nouns as \"thngs\"? We need moree rigorouss teckneques to define\n",
            "these notioms.\n",
            "*** Min distance + Frequency result:  It is widely taught that parts of speech are defined in terms of simple definitions for example a noun is a person place or thing but in reality simple definitions like that are not very useful for example is it not true that everything is a thing then in what sense can we define nouns as things we need more rigorous techniques to define these notions\n",
            "*** Final result: It is widely taught that parts of speech are defined in terms of simple definitions for example any noun is any person place or thing but in reality simple definitions like that are not very useful for example is it not true that everything is any thing then in what sense can we define nouns as things we need more rigorous techniques to define these notions\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ooL_nQk2U1Q_",
        "XSwAF2zwU1RD",
        "8GC1rZA5eCYw",
        "8Jz4fEoPhRNL",
        "C_HUJOk61hlc",
        "LjduS0gF_Nxv"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}