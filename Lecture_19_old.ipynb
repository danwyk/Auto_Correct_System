{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"GKgNUpzzFROm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668027971888,"user_tz":300,"elapsed":4789,"user":{"displayName":"Daniel Wang","userId":"12576965196986841416"}},"outputId":"25839133-b9a4-4ee5-9e54-033b25cc3a3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["nltk_stuff.zip      100%[===================>]   2.41M  --.-KB/s    in 0.01s   \n","Class 7.zip         100%[===================>]   1.04M  --.-KB/s    in 0.01s   \n"]}],"source":["# LOAD THE FILES FOR THIS NOTEBOOK\n","!wget -q --show-progress --no-check-certificate 'https://docs.google.com/uc?export=download&id=1sVJ9Tq6KYxf3_DzWiDJoxnMgghHo22Md' -O 'nltk_stuff.zip'\n","from zipfile import ZipFile\n","with ZipFile('nltk_stuff.zip', 'r') as zipObj:\n","  zipObj.extractall('../root/nltk_data/taggers')\n","\n","!wget -q --show-progress --no-check-certificate 'https://docs.google.com/uc?export=download&id=1cuIbCCZZutqlO-d9s9KxI6GxfbHd8rnb' -O 'Class 7.zip'\n","from zipfile import ZipFile\n","with ZipFile('Class 7.zip', 'r') as zipObj:\n","  zipObj.extractall()"]},{"cell_type":"markdown","source":["<b>LING 193 - Lecture 19<br>\n","Basic syntax applications in Python</b><br>\n","Andrew McInnerney<br>\n","November 7, 2022"],"metadata":{"id":"FDIZeF-uFakS"}},{"cell_type":"markdown","source":["First, I'm loading in our autocorrect functions from earlier in the course."],"metadata":{"id":"3OkS73qJTeqG"}},{"cell_type":"code","source":["def firstrow(m):\n","    return [i for i in range(m+1)]\n","\n","def substitution_penalty(letter1, letter2):\n","    if letter1 == letter2:\n","        return 0\n","    else:\n","        return 2\n","\n","def nextrow(priorrow, word1, letter):\n","    row = [priorrow[0] + 1]\n","    priorcell = row[0]\n","    for i in range(len(word1)):\n","        insertion = priorrow[i+1] + 1\n","        deletion = priorcell + 1\n","        substitution = priorrow[i] + substitution_penalty(word1[i], letter)\n","        priorcell = min(insertion, deletion, substitution)\n","        row.append(priorcell)\n","    return row\n","\n","def minedit(word1, word2):\n","    m = len(word1)\n","    n = len(word2)\n","    priorrow = firstrow(m)\n","    table = [priorrow]\n","    for i in range(n):\n","        row = nextrow(priorrow, word1, word2[i])\n","        table.append(row)\n","        priorrow = row\n","    return table\n","\n","def citformat(word):\n","  punctuation = [\"!\",\"?\",\",\",\".\",\"-\",\"~\",\":\",\";\", \"'\", '\"']\n","  while word[-1] in punctuation:\n","    word = word[:-1]\n","  while word[0] in punctuation:\n","    word = word[1:]\n","  return word.upper()\n","\n","with open(\"subtlex_words.txt\") as file: # This changes our dictionary to a shorter one of more frequent words\n","    words = file.read().splitlines()\n","frequencies = {}\n","for entry in words:\n","    word = entry.split()[0].upper()\n","    freq = int(entry.split()[1])\n","    if word not in frequencies:\n","        frequencies[word] = freq\n","dictionary = {word:freq for word,freq in frequencies.items() if freq > 12}\n","\n","def correct(typo):\n","    errors = {}\n","    mindist = 100\n","    for word in dictionary:\n","        table = minedit(typo,word)\n","        distance = table[-1][-1]\n","        if distance < mindist:\n","          errors = {}\n","          mindist = distance\n","        if distance <= mindist:\n","          errors[word] = distance\n","    return max(errors, key = dictionary.get)"],"metadata":{"id":"QAYfb4-TTZ99","executionInfo":{"status":"ok","timestamp":1668028120317,"user_tz":300,"elapsed":330,"user":{"displayName":"Daniel Wang","userId":"12576965196986841416"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Creating some simple sentences of the form:\n","\n","Det N V Det N<br>\n","N Aux Adv V Det Adj N<br>\n","N Aux V P Det N"],"metadata":{"id":"1C3CwPI8GNJY"}},{"cell_type":"code","source":["s1 = \"The manager hired ths employees\".split()\n","s2 = \"John will not buy anl tomatoes\".split()\n","s3 = \"We are going to take ix classes\".split()"],"metadata":{"id":"DmIH62b7vBie","executionInfo":{"status":"ok","timestamp":1668028129399,"user_tz":300,"elapsed":194,"user":{"displayName":"Daniel Wang","userId":"12576965196986841416"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["\"Correcting\" these sentences:"],"metadata":{"id":"PhZktML53A2u"}},{"cell_type":"code","source":["for i in range(len(s1)):\n","  s1[i] = correct(s1[i].upper()).lower()\n","print(\" \".join(s1).capitalize())\n","\n","for i in range(len(s2)):\n","  s2[i] = correct(s2[i].upper()).lower()\n","print(\" \".join(s2).capitalize())\n","\n","for i in range(len(s3)):\n","  s3[i] = correct(s3[i].upper()).lower()\n","print(\" \".join(s3).capitalize())"],"metadata":{"id":"gSLe6tq_3DW1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668028146176,"user_tz":300,"elapsed":14357,"user":{"displayName":"Daniel Wang","userId":"12576965196986841416"}},"outputId":"6a183d4f-a18a-43fd-9f24-73f20f101f9e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["The manager hired this employees\n","John will not buy an tomatoes\n","We are going to take i classes\n"]}]},{"cell_type":"markdown","source":["# 2 Part-of-Speech tagging\n","\n","Notice that some of the words aren't corrected accurately. \n","- The word *ths* should be corrected to *the*, not *ths*. \n","- The word *anl* should be corrected to *any*, not *an*.\n","- The word *ix* should be corrected to *six*, not *i*.\n","\n","We could take various approaches to solve each of these problems. For the word *an*, for instance, we could create a special rule that says *an* should only be considered if the next word starts with a vowel.\n","\n","For the other examples, we may want to consider part of speech tagging. Take the first sentence. We could penalize *this* as a possible correction if the following noun is plural.\n","\n","To do this, we would need part-of-speech information. We can get this from the Natural Language Toolkit (nltk). Below, we load in nltk, along with the part-of-speech tagger we need."],"metadata":{"id":"rfgpBYXhVrPl"}},{"cell_type":"code","source":["# Installing nltk into this notebook\n","%pip install nltk"],"metadata":{"id":"Vq_YL3a-XDM4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668029230524,"user_tz":300,"elapsed":3740,"user":{"displayName":"Daniel Wang","userId":"12576965196986841416"}},"outputId":"18a0d414-19f4-484d-bb05-92395f181b36"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n"]}]},{"cell_type":"code","source":["# Importing into Python\n","import nltk"],"metadata":{"id":"G-tUge9WXJF-","executionInfo":{"status":"ok","timestamp":1668029237134,"user_tz":300,"elapsed":2285,"user":{"displayName":"Daniel Wang","userId":"12576965196986841416"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Now take a look at the information we get from our control sentences, using the `pos_tag()` method:"],"metadata":{"id":"iJ8ZOL7dXK8u"}},{"cell_type":"code","source":["s1 = \"The manager hired the employees\".split()\n","s2 = \"John will not buy any tomatoes\".split()\n","s3 = \"We are going to take six classes\".split()\n","\n","print(nltk.pos_tag(s1))\n","print(nltk.pos_tag(s2))\n","print(nltk.pos_tag(s3))"],"metadata":{"id":"TUxrI7MJXN7e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668029259157,"user_tz":300,"elapsed":182,"user":{"displayName":"Daniel Wang","userId":"12576965196986841416"}},"outputId":"8fec0d63-09d2-4210-9dfe-1b21645b3960"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[('The', 'DT'), ('manager', 'NN'), ('hired', 'VBD'), ('the', 'DT'), ('employees', 'NNS')]\n","[('John', 'NNP'), ('will', 'MD'), ('not', 'RB'), ('buy', 'VB'), ('any', 'DT'), ('tomatoes', 'NNS')]\n","[('We', 'PRP'), ('are', 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('take', 'VB'), ('six', 'CD'), ('classes', 'NNS')]\n"]}]},{"cell_type":"markdown","source":["And here's what we get for our test sentences:"],"metadata":{"id":"EN1qdEoAXVCZ"}},{"cell_type":"code","source":["s1 = \"The manager hired ths employees\".split()\n","s2 = \"John will not buy anl tomatoes\".split()\n","s3 = \"We are going to take ix classes\".split()\n","\n","for i in range(len(s1)):\n","  s1[i] = correct(s1[i].upper()).lower()\n","\n","for i in range(len(s2)):\n","  s2[i] = correct(s2[i].upper()).lower()\n","\n","for i in range(len(s3)):\n","  s3[i] = correct(s3[i].upper()).lower()\n","\n","print(nltk.pos_tag(s1))\n","print(nltk.pos_tag(s2))\n","print(nltk.pos_tag(s3))"],"metadata":{"id":"KFodKPFfXjvi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668029283601,"user_tz":300,"elapsed":16111,"user":{"displayName":"Daniel Wang","userId":"12576965196986841416"}},"outputId":"1bc93965-201d-48e2-c462-ba6f294046a3"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[('the', 'DT'), ('manager', 'NN'), ('hired', 'VBD'), ('this', 'DT'), ('employees', 'NNS')]\n","[('john', 'NN'), ('will', 'MD'), ('not', 'RB'), ('buy', 'VB'), ('an', 'DT'), ('tomatoes', 'NN')]\n","[('we', 'PRP'), ('are', 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('take', 'VB'), ('i', 'NN'), ('classes', 'NNS')]\n"]}]},{"cell_type":"markdown","source":["To correct *ths* to *the* instead of *this*, we need to penalize the word *this* in the context of a whole sentence, taking into account the part-of-speech of the following word. We would want to define a function `autocorrect()` that takes into account not only the minedit distance between two words, but also rules about which categories can occur where. "],"metadata":{"id":"ARo727fkZF_7"}},{"cell_type":"markdown","source":["# 3 Bigrams\n","\n","To solve our third error (correcting *ix* to *six* instead of *i*), we need to refer to *bigrams*. We can get the information we need from nltk's corpora. We will specifically use teh Brown Corpus:"],"metadata":{"id":"bXC3pL-HY0GQ"}},{"cell_type":"code","source":["nltk.download('brown')\n","from nltk.corpus import brown\n","print(\"the brown corpus contains\",len(brown.fileids()),\"texts with a total of\",len(brown.sents()),\"sentences and\",len(brown.words()),\"words\")\n","print(\"there are an average of\",len(brown.words())/len(brown.sents()),\"words per sentence\")"],"metadata":{"id":"jmt9q3B7IqeE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668029618788,"user_tz":300,"elapsed":10305,"user":{"displayName":"Daniel Wang","userId":"12576965196986841416"}},"outputId":"3ed91aa6-81b7-4c72-ba54-2a9ce0399cb6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n"]},{"output_type":"stream","name":"stdout","text":["the brown corpus contains 500 texts with a total of 57340 sentences and 1161192 words\n","there are an average of 20.250994070456922 words per sentence\n"]}]},{"cell_type":"markdown","source":["Here I'm turning all the sentences in the Brown Corpus into tagged lists."],"metadata":{"id":"WuGerfVfaHFk"}},{"cell_type":"code","source":["browncorp = []\n","for text in brown.fileids():\n","  for sentence in brown.sents(text):\n","    browncorp.append(nltk.pos_tag(sentence))\n","print(\"The browncorp is a list of all the sentences of the brown corpus with part-of-speech tags.\")\n","print(\"Example:\")\n","print(browncorp[0])"],"metadata":{"id":"LOPBS4h_HQpo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668029717385,"user_tz":300,"elapsed":58265,"user":{"displayName":"Daniel Wang","userId":"12576965196986841416"}},"outputId":"205145a2-3865-464f-cf52-7f7658844bbb"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["The browncorp is a list of all the sentences of the brown corpus with part-of-speech tags.\n","Example:\n","[('The', 'DT'), ('Fulton', 'NNP'), ('County', 'NNP'), ('Grand', 'NNP'), ('Jury', 'NNP'), ('said', 'VBD'), ('Friday', 'NNP'), ('an', 'DT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NNP'), ('recent', 'JJ'), ('primary', 'JJ'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'DT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'IN'), ('any', 'DT'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n"]}]},{"cell_type":"markdown","source":["Now we can calculate part-of-speech bigrams."],"metadata":{"id":"4hlb3QkiNc87"}},{"cell_type":"code","source":["bigrams = {} # Create an empty dictionary\n","for sentence in browncorp: # Look at each tagged sentence in browncorp\n","  for i in range(len(sentence)-1): # We iterate over each word except the last one\n","    currentword = sentence[i] # The current word is the one at position i\n","    nextword = sentence[i+1] # The next word is the one at position i+1\n","    bigram = currentword[1]+\"_\"+nextword[1] # The current bigram is the part of speech label from those two words\n","    if bigram in bigrams: # If the current bigram is already a key in the bigrams dictionary\n","      bigrams[bigram] += 1 # Then we increment the count for that bigram\n","    else:\n","      bigrams[bigram] = 1 # Otherwise we create an entry for that bigram\n","\n","print(\"We have identified\",len(bigrams),\"bigrams\")\n","\n","# for bigram in bigrams:\n","#   print(f\"{bigram}: {bigrams[bigram]}\")\n"],"metadata":{"id":"0lY_90tWM-8v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668030263106,"user_tz":300,"elapsed":402,"user":{"displayName":"Daniel Wang","userId":"12576965196986841416"}},"outputId":"bf7c719e-2017-42a8-f82f-4a069e8b2ca2"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["We have identified 1345 bigrams\n"]}]},{"cell_type":"markdown","source":["Let's take a look at the most and least common of these:"],"metadata":{"id":"1M6L5S5rO2OX"}},{"cell_type":"code","source":["sortedbigrams = sorted(bigrams, key=bigrams.get, reverse = True)\n","for bigram in sortedbigrams[0:10]:\n","  print(bigram, bigrams[bigram])\n","print(\"...\")\n","for bigram in sortedbigrams[:-10:-1]:\n","  print(bigram, bigrams[bigram])"],"metadata":{"id":"yOpgGNmWNZpt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668030236964,"user_tz":300,"elapsed":196,"user":{"displayName":"Daniel Wang","userId":"12576965196986841416"}},"outputId":"9fd7d97c-9049-4d83-fb65-e57e5b8c1622"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["DT_NN 55590\n","IN_DT 52178\n","NN_IN 45328\n","JJ_NN 36223\n","DT_JJ 28839\n","NN_. 20018\n","NN_, 19476\n","NNP_NNP 18879\n","IN_NN 17756\n","JJ_NNS 16036\n","...\n","POS_VBG 1\n","FW_CD 1\n","UH_VB 1\n","NNPS_WP$ 1\n","MD_VBZ 1\n","PRP$_EX 1\n","UH_PRP$ 1\n","``_POS 1\n","JJ_UH 1\n"]}]},{"cell_type":"markdown","source":["Let's keep only the most frequent bigrams:"],"metadata":{"id":"JPWsmePyQfIM"}},{"cell_type":"code","source":["sortedbigrams = sortedbigrams[:700]\n","for bigram in sortedbigrams[0:10]:\n","  print(bigram, bigrams[bigram])\n","print(\"...\")\n","for bigram in sortedbigrams[:-10:-1]:\n","  print(bigram, bigrams[bigram])"],"metadata":{"id":"ooQJoLqIQeul","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668030288102,"user_tz":300,"elapsed":410,"user":{"displayName":"Daniel Wang","userId":"12576965196986841416"}},"outputId":"b8eb85c7-a863-4689-a552-cd3182a9e0c4"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["DT_NN 55590\n","IN_DT 52178\n","NN_IN 45328\n","JJ_NN 36223\n","DT_JJ 28839\n","NN_. 20018\n","NN_, 19476\n","NNP_NNP 18879\n","IN_NN 17756\n","JJ_NNS 16036\n","...\n","CD_WDT 46\n","WRB_VBP 46\n",",_UH 47\n","NNS_WP$ 47\n","VBN_VBP 47\n","JJS_. 47\n","WRB_EX 47\n",":_WP 47\n","NNPS_'' 47\n"]}]},{"cell_type":"markdown","source":["To get the right correction in `s3` (i.e. turning *ix* into *six* not *i*), we need to take bigram frequency into account in our `autocorrect()` function. We should have some logic that boosts the score of a particular word if it creates a more frequent bigram.\n","\n","Another strategy is to use *trigrams*. There will be many more of these to calculate, but the principles are basically the same."],"metadata":{"id":"_ztRiqyBaWr1"}}]}