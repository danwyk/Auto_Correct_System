{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"GKgNUpzzFROm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669078470835,"user_tz":300,"elapsed":96774,"user":{"displayName":"Daniel Wang","userId":"12576965196986841416"}},"outputId":"dcbf8754-c544-4f3a-9cc5-c19801aa830e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\rClass 7.zip           0%[                    ]       0  --.-KB/s               \rClass 7.zip         100%[===================>]   1.04M  --.-KB/s    in 0.01s   \n","--2022-11-22 00:52:56--  https://nlp.stanford.edu/software/stanford-corenlp-4.5.1.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 302 FOUND\n","Location: https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-4.5.1.zip [following]\n","--2022-11-22 00:52:56--  https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-4.5.1.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 505225173 (482M) [application/zip]\n","Saving to: ‘stanford-corenlp-4.5.1.zip’\n","\n","stanford-corenlp-4. 100%[===================>] 481.82M  4.97MB/s    in 91s     \n","\n","2022-11-22 00:54:28 (5.28 MB/s) - ‘stanford-corenlp-4.5.1.zip’ saved [505225173/505225173]\n","\n"]}],"source":["# LOAD THE FILES FOR THIS NOTEBOOK\n","!wget -q --show-progress --no-check-certificate 'https://docs.google.com/uc?export=download&id=1cuIbCCZZutqlO-d9s9KxI6GxfbHd8rnb' -O 'Class 7.zip'\n","from zipfile import ZipFile\n","with ZipFile('Class 7.zip', 'r') as zipObj:\n","  zipObj.extractall()\n","\n","!wget https://nlp.stanford.edu/software/stanford-corenlp-4.5.1.zip\n","with ZipFile('stanford-corenlp-4.5.1.zip', 'r') as zipObj:\n","  zipObj.extractall()"]},{"cell_type":"markdown","source":["<b>LING 193 - Lecture 20<br>\n","Basic syntax applications in Python</b><br>\n","Andrew McInnerney<br>\n","November 9, 2022"],"metadata":{"id":"FDIZeF-uFakS"}},{"cell_type":"markdown","source":["First, I'm loading in our autocorrect functions from earlier in the course."],"metadata":{"id":"3OkS73qJTeqG"}},{"cell_type":"code","source":["def firstrow(m):\n","    return [i for i in range(m+1)]\n","\n","def substitution_penalty(letter1, letter2):\n","    if letter1 == letter2:\n","        return 0\n","    else:\n","        return 2\n","\n","def nextrow(priorrow, word1, letter):\n","    row = [priorrow[0] + 1]\n","    priorcell = row[0]\n","    for i in range(len(word1)):\n","        insertion = priorrow[i+1] + 1\n","        deletion = priorcell + 1\n","        substitution = priorrow[i] + substitution_penalty(word1[i], letter)\n","        priorcell = min(insertion, deletion, substitution)\n","        row.append(priorcell)\n","    return row\n","\n","def minedit(word1, word2):\n","    m = len(word1)\n","    n = len(word2)\n","    priorrow = firstrow(m)\n","    table = [priorrow]\n","    for i in range(n):\n","        row = nextrow(priorrow, word1, word2[i])\n","        table.append(row)\n","        priorrow = row\n","    return table\n","\n","def citformat(word):\n","  punctuation = [\"!\",\"?\",\",\",\".\",\"-\",\"~\",\":\",\";\", \"'\", '\"']\n","  while word[-1] in punctuation:\n","    word = word[:-1]\n","  while word[0] in punctuation:\n","    word = word[1:]\n","  return word.upper()\n","\n","with open(\"subtlex_words.txt\") as file: # This changes our dictionary to a shorter one of more frequent words\n","    words = file.read().splitlines()\n","frequencies = {}\n","for entry in words:\n","    word = entry.split()[0].upper()\n","    freq = int(entry.split()[1])\n","    if word not in frequencies:\n","        frequencies[word] = freq\n","dictionary = {word:freq for word,freq in frequencies.items() if freq > 12}\n","\n","def correct(typo):\n","    errors = {}\n","    mindist = 100\n","    for word in dictionary:\n","        table = minedit(typo,word)\n","        distance = table[-1][-1]\n","        if distance < mindist:\n","          errors = {}\n","          mindist = distance\n","        if distance <= mindist:\n","          errors[word] = distance\n","    return max(errors, key = dictionary.get)"],"metadata":{"id":"QAYfb4-TTZ99","executionInfo":{"status":"ok","timestamp":1669081766252,"user_tz":300,"elapsed":513,"user":{"displayName":"Daniel Wang","userId":"12576965196986841416"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Creating some simple sentences of the form:\n","\n","Det N V Det N<br>\n","N Aux Adv V Det Adj N<br>\n","N Aux V P Det N"],"metadata":{"id":"1C3CwPI8GNJY"}},{"cell_type":"code","source":["s1 = \"The manager hired ths employees\".split()\n","s2 = \"John will not buy anu tomatoes\".split()\n","s3 = \"We are going to take ix classes\".split()"],"metadata":{"id":"DmIH62b7vBie","executionInfo":{"status":"ok","timestamp":1669081769381,"user_tz":300,"elapsed":128,"user":{"displayName":"Daniel Wang","userId":"12576965196986841416"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["\"Correcting\" these sentences:"],"metadata":{"id":"PhZktML53A2u"}},{"cell_type":"code","source":["for i in range(len(s1)):\n","  s1[i] = correct(s1[i].upper()).lower()\n","print(\" \".join(s1).capitalize())\n","\n","for i in range(len(s2)):\n","  s2[i] = correct(s2[i].upper()).lower()\n","print(\" \".join(s2).capitalize())\n","\n","for i in range(len(s3)):\n","  s3[i] = correct(s3[i].upper()).lower()\n","print(\" \".join(s3).capitalize())"],"metadata":{"id":"gSLe6tq_3DW1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5976e045-9591-4df5-a457-09c422aad31a","executionInfo":{"status":"ok","timestamp":1669081786682,"user_tz":300,"elapsed":16651,"user":{"displayName":"Daniel Wang","userId":"12576965196986841416"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["The manager hired this employees\n","John will not buy an tomatoes\n","We are going to take i classes\n"]}]},{"cell_type":"markdown","source":["# 2 Part-of-Speech tagging\n","\n","Notice that some of the words aren't corrected accurately. \n","- The word *ths* should be corrected to *the*, not *ths*. \n","- The word *anl* should be corrected to *any*, not *an*.\n","- The word *ix* should be corrected to *six*, not *i*.\n","\n","We could take various approaches to solve each of these problems. For the word *an*, for instance, we could create a special rule that says *an* should only be considered if the next word starts with a vowel.\n","\n","For the other examples, we may want to consider part of speech tagging. Take the first sentence. We could penalize *this* as a possible correction if the following noun is plural.\n","\n","To do this, we would need part-of-speech information. We can get this from the Natural Language Toolkit (nltk). Below, we load in nltk, along with the part-of-speech tagger we need."],"metadata":{"id":"rfgpBYXhVrPl"}},{"cell_type":"code","source":["# Installing stanford's Core NLP into this notebook\n","%pip install stanfordcorenlp"],"metadata":{"id":"Vq_YL3a-XDM4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importing into Python\n","from stanfordcorenlp import StanfordCoreNLP\n","nlp = StanfordCoreNLP('stanford-corenlp-4.5.1')"],"metadata":{"id":"G-tUge9WXJF-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now take a look at the information we get from our control sentences, using the `pos_tag()` method:"],"metadata":{"id":"iJ8ZOL7dXK8u"}},{"cell_type":"code","source":["s1 = \"The manager hired the employees.\"\n","s2 = \"John will not buy any tomatoes.\"\n","s3 = \"We are going to take six classes.\"\n","\n","print(nlp.pos_tag(s1))\n","print(nlp.pos_tag(s2))\n","print(nlp.pos_tag(s3))"],"metadata":{"id":"TUxrI7MJXN7e","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f6921ac2-3c28-4506-dcff-c48a5b7c5ae5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('The', 'DT'), ('manager', 'NN'), ('hired', 'VBD'), ('the', 'DT'), ('employees', 'NNS'), ('.', '.')]\n","[('John', 'NNP'), ('will', 'MD'), ('not', 'RB'), ('buy', 'VB'), ('any', 'DT'), ('tomatoes', 'NNS'), ('.', '.')]\n","[('We', 'PRP'), ('are', 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('take', 'VB'), ('six', 'CD'), ('classes', 'NNS'), ('.', '.')]\n"]}]},{"cell_type":"markdown","source":["And here's what we get for our test sentences:"],"metadata":{"id":"EN1qdEoAXVCZ"}},{"cell_type":"code","source":["s1 = \"The manager hired ths employees\".split()\n","s2 = \"John will not buy anl tomatoes\".split()\n","s3 = \"We are going to take ix classes\".split()\n","\n","for i in range(len(s1)):\n","  s1[i] = correct(s1[i].upper()).lower()\n","s1 = \" \".join(s1)\n","\n","for i in range(len(s2)):\n","  s2[i] = correct(s2[i].upper()).lower()\n","s2 = \" \".join(s2)\n","\n","for i in range(len(s3)):\n","  s3[i] = correct(s3[i].upper()).lower()\n","s3 = ' '.join(s3)\n","\n","print(nlp.pos_tag(s1))\n","print(nlp.pos_tag(s2))\n","print(nlp.pos_tag(s3))"],"metadata":{"id":"KFodKPFfXjvi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1ed65d85-dacf-4ecf-b593-b40ca533fb4b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('the', 'DT'), ('manager', 'NN'), ('hired', 'VBD'), ('this', 'DT'), ('employees', 'NNS')]\n","[('john', 'NNP'), ('will', 'MD'), ('not', 'RB'), ('buy', 'VB'), ('an', 'DT'), ('tomatoes', 'NNS')]\n","[('we', 'PRP'), ('are', 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('take', 'VB'), ('i', 'PRP'), ('classes', 'NNS')]\n"]}]},{"cell_type":"markdown","source":["Note: You can find a list of part-of-speech tags [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n","\n","To correct *ths* to *the* instead of *this*, we need to penalize the word *this* in the context of a whole sentence, taking into account the part-of-speech of the following word. We would want to define a function `autocorrect()` that takes into account not only the minedit distance between two words, but also rules about which categories can occur where. If you choose the coding option for your final project, you might consider implementing some rules like the ones "],"metadata":{"id":"ARo727fkZF_7"}},{"cell_type":"markdown","source":["# 3 Bigrams\n","\n","To solve our third error (correcting *ix* to *six* instead of *i*), we need to refer to *bigrams*. We can get the information we need from nltk's corpora. We will specifically use teh Brown Corpus:"],"metadata":{"id":"bXC3pL-HY0GQ"}},{"cell_type":"code","source":["%pip install nltk"],"metadata":{"id":"_8DKAmC2ALRM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3cdbc06f-9ed7-4728-83a6-5d63b2e8a800"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('brown')\n","from nltk.corpus import brown\n","print(\"the brown corpus contains\",len(brown.fileids()),\"texts with a total of\",len(brown.sents()),\"sentences and\",len(brown.words()),\"words\")\n","print(\"there are an average of\",len(brown.words())/len(brown.sents()),\"words per sentence\")"],"metadata":{"id":"jmt9q3B7IqeE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3ca63fde-b369-4c14-efa4-52e0f65fe7e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n"]},{"output_type":"stream","name":"stdout","text":["the brown corpus contains 500 texts with a total of 57340 sentences and 1161192 words\n","there are an average of 20.250994070456922 words per sentence\n"]}]},{"cell_type":"markdown","source":["Here I'm turning all the sentences in the Brown Corpus into tagged lists."],"metadata":{"id":"WuGerfVfaHFk"}},{"cell_type":"code","source":["browntags = brown.tagged_sents()\n","print(\"The browntags object is a list of all the sentences of the brown corpus with part-of-speech tags.\")\n","print(\"Example:\")\n","for i in browntags[:10]:\n","  print(i)"],"metadata":{"id":"LOPBS4h_HQpo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"45924399-133f-4eff-fe1d-68a41d648450"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The browntags object is a list of all the sentences of the brown corpus with part-of-speech tags.\n","Example:\n","[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n","[('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')]\n","[('The', 'AT'), ('September-October', 'NP'), ('term', 'NN'), ('jury', 'NN'), ('had', 'HVD'), ('been', 'BEN'), ('charged', 'VBN'), ('by', 'IN'), ('Fulton', 'NP-TL'), ('Superior', 'JJ-TL'), ('Court', 'NN-TL'), ('Judge', 'NN-TL'), ('Durwood', 'NP'), ('Pye', 'NP'), ('to', 'TO'), ('investigate', 'VB'), ('reports', 'NNS'), ('of', 'IN'), ('possible', 'JJ'), ('``', '``'), ('irregularities', 'NNS'), (\"''\", \"''\"), ('in', 'IN'), ('the', 'AT'), ('hard-fought', 'JJ'), ('primary', 'NN'), ('which', 'WDT'), ('was', 'BEDZ'), ('won', 'VBN'), ('by', 'IN'), ('Mayor-nominate', 'NN-TL'), ('Ivan', 'NP'), ('Allen', 'NP'), ('Jr.', 'NP'), ('.', '.')]\n","[('``', '``'), ('Only', 'RB'), ('a', 'AT'), ('relative', 'JJ'), ('handful', 'NN'), ('of', 'IN'), ('such', 'JJ'), ('reports', 'NNS'), ('was', 'BEDZ'), ('received', 'VBN'), (\"''\", \"''\"), (',', ','), ('the', 'AT'), ('jury', 'NN'), ('said', 'VBD'), (',', ','), ('``', '``'), ('considering', 'IN'), ('the', 'AT'), ('widespread', 'JJ'), ('interest', 'NN'), ('in', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('the', 'AT'), ('number', 'NN'), ('of', 'IN'), ('voters', 'NNS'), ('and', 'CC'), ('the', 'AT'), ('size', 'NN'), ('of', 'IN'), ('this', 'DT'), ('city', 'NN'), (\"''\", \"''\"), ('.', '.')]\n","[('The', 'AT'), ('jury', 'NN'), ('said', 'VBD'), ('it', 'PPS'), ('did', 'DOD'), ('find', 'VB'), ('that', 'CS'), ('many', 'AP'), ('of', 'IN'), (\"Georgia's\", 'NP$'), ('registration', 'NN'), ('and', 'CC'), ('election', 'NN'), ('laws', 'NNS'), ('``', '``'), ('are', 'BER'), ('outmoded', 'JJ'), ('or', 'CC'), ('inadequate', 'JJ'), ('and', 'CC'), ('often', 'RB'), ('ambiguous', 'JJ'), (\"''\", \"''\"), ('.', '.')]\n","[('It', 'PPS'), ('recommended', 'VBD'), ('that', 'CS'), ('Fulton', 'NP'), ('legislators', 'NNS'), ('act', 'VB'), ('``', '``'), ('to', 'TO'), ('have', 'HV'), ('these', 'DTS'), ('laws', 'NNS'), ('studied', 'VBN'), ('and', 'CC'), ('revised', 'VBN'), ('to', 'IN'), ('the', 'AT'), ('end', 'NN'), ('of', 'IN'), ('modernizing', 'VBG'), ('and', 'CC'), ('improving', 'VBG'), ('them', 'PPO'), (\"''\", \"''\"), ('.', '.')]\n","[('The', 'AT'), ('grand', 'JJ'), ('jury', 'NN'), ('commented', 'VBD'), ('on', 'IN'), ('a', 'AT'), ('number', 'NN'), ('of', 'IN'), ('other', 'AP'), ('topics', 'NNS'), (',', ','), ('among', 'IN'), ('them', 'PPO'), ('the', 'AT'), ('Atlanta', 'NP'), ('and', 'CC'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('purchasing', 'VBG'), ('departments', 'NNS'), ('which', 'WDT'), ('it', 'PPS'), ('said', 'VBD'), ('``', '``'), ('are', 'BER'), ('well', 'QL'), ('operated', 'VBN'), ('and', 'CC'), ('follow', 'VB'), ('generally', 'RB'), ('accepted', 'VBN'), ('practices', 'NNS'), ('which', 'WDT'), ('inure', 'VB'), ('to', 'IN'), ('the', 'AT'), ('best', 'JJT'), ('interest', 'NN'), ('of', 'IN'), ('both', 'ABX'), ('governments', 'NNS'), (\"''\", \"''\"), ('.', '.')]\n","[('Merger', 'NN-HL'), ('proposed', 'VBN-HL')]\n","[('However', 'WRB'), (',', ','), ('the', 'AT'), ('jury', 'NN'), ('said', 'VBD'), ('it', 'PPS'), ('believes', 'VBZ'), ('``', '``'), ('these', 'DTS'), ('two', 'CD'), ('offices', 'NNS'), ('should', 'MD'), ('be', 'BE'), ('combined', 'VBN'), ('to', 'TO'), ('achieve', 'VB'), ('greater', 'JJR'), ('efficiency', 'NN'), ('and', 'CC'), ('reduce', 'VB'), ('the', 'AT'), ('cost', 'NN'), ('of', 'IN'), ('administration', 'NN'), (\"''\", \"''\"), ('.', '.')]\n","[('The', 'AT'), ('City', 'NN-TL'), ('Purchasing', 'VBG-TL'), ('Department', 'NN-TL'), (',', ','), ('the', 'AT'), ('jury', 'NN'), ('said', 'VBD'), (',', ','), ('``', '``'), ('is', 'BEZ'), ('lacking', 'VBG'), ('in', 'IN'), ('experienced', 'VBN'), ('clerical', 'JJ'), ('personnel', 'NNS'), ('as', 'CS'), ('a', 'AT'), ('result', 'NN'), ('of', 'IN'), ('city', 'NN'), ('personnel', 'NNS'), ('policies', 'NNS'), (\"''\", \"''\"), ('.', '.')]\n"]}]},{"cell_type":"markdown","source":["Now we can calculate part-of-speech bigrams."],"metadata":{"id":"4hlb3QkiNc87"}},{"cell_type":"code","source":["bigrams = {} # Create an empty dictionary\n","for sentence in browntags: # Look at each tagged sentence in browncorp\n","  for i in range(len(sentence)-1): # We iterate over each word except the last one\n","    currentword = sentence[i] # The current word is the one at position i\n","    nextword = sentence[i+1] # The next word is the one at position i+1\n","    bigram = currentword[1]+\"_\"+nextword[1] # The current bigram is the part of speech label from those two words\n","    if bigram in bigrams: # If the current bigram is already a key in the bigrams dictionary\n","      bigrams[bigram] += 1 # Then we increment the count for that bigram\n","    else:\n","      bigrams[bigram] = 1 # Otherwise we create an entry for that bigram\n","\n","print(\"We have identified\",len(bigrams),\"bigrams\")"],"metadata":{"id":"0lY_90tWM-8v","colab":{"base_uri":"https://localhost:8080/"},"outputId":"14e6a46c-0c70-422c-e51a-e321a73abc59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["We have identified 8053 bigrams\n"]}]},{"cell_type":"markdown","source":["Let's take a look at the most and least common of these:"],"metadata":{"id":"1M6L5S5rO2OX"}},{"cell_type":"code","source":["sortedbigrams = sorted(bigrams, key=bigrams.get, reverse = True)\n","for bigram in sortedbigrams[0:10]:\n","  print(bigram, bigrams[bigram])\n","print(\"...\")\n","for bigram in sortedbigrams[:-10:-1]:\n","  print(bigram, bigrams[bigram])"],"metadata":{"id":"yOpgGNmWNZpt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"92fe287f-97be-43ac-9916-905a7a879486"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["AT_NN 48372\n","IN_AT 43271\n","NN_IN 42252\n","JJ_NN 28407\n","NN_. 19857\n","AT_JJ 19487\n","NN_, 18279\n","IN_NN 17225\n","NNS_IN 14504\n","TO_VB 12291\n","...\n","FW-NN_QL 1\n","BE_FW-RB 1\n","PPO_BEZ* 1\n","FW-PP$-TL_'' 1\n","FW-NN-TL_FW-PP$-TL 1\n","FW-UH-TL_FW-NN-TL 1\n","``_FW-UH-TL 1\n","NP$-TL_CC-TL 1\n","PPO_FW-NN 1\n"]}]},{"cell_type":"markdown","source":["Let's keep only the most frequent bigrams:"],"metadata":{"id":"JPWsmePyQfIM"}},{"cell_type":"code","source":["sortedbigrams = sortedbigrams[:800]\n","for bigram in sortedbigrams[:10]:\n","  print(bigram, bigrams[bigram])\n","print(\"...\")\n","for bigram in sortedbigrams[-10:]:\n","  print(bigram, bigrams[bigram])"],"metadata":{"id":"ooQJoLqIQeul"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To get the right correction in `s3` (i.e. turning *ix* into *six* not *i*), we need to take bigram frequency into account in our `autocorrect()` function. We should have some logic that boosts the score of a particular word if it creates a more frequent bigram.\n","\n","Another strategy is to use *trigrams*. There will be many more of these to calculate, but the principles are basically the same."],"metadata":{"id":"_ztRiqyBaWr1"}},{"cell_type":"markdown","source":["# Parsing\n","Here we will look at Stanford's `stanza` package. First we need to bring some files into Google Colab, which we can do with this code:"],"metadata":{"id":"HMGvUOGHycmD"}},{"cell_type":"code","source":["%pip install stanza\n","import stanza\n","stanza.download('en')\n","nlp = stanza.Pipeline('en',processors='tokenize,pos,constituency')"],"metadata":{"id":"waO5Clkk8TJI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can use stanza to generate constituent structures for example sentences. Here is some code to do that:"],"metadata":{"id":"2tCzmWa0YOH9"}},{"cell_type":"code","source":["doc = nlp('I am watching my computer do something interesting. It is great.')\n","for sentence in doc.sentences:\n","  structure = sentence.constituency.children\n","  print(structure)"],"metadata":{"id":"jXoir4Pv8tns","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2b28f633-236a-43b8-87d3-cbac016c61b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["((S (NP (PRP I)) (VP (VBP am) (VP (VBG watching) (S (NP (PRP$ my) (NN computer)) (VP (VB do) (NP (NP (NN something)) (ADJP (JJ interesting))))))) (. .)),)\n","((S (NP (PRP It)) (VP (VBZ is) (ADJP (JJ great))) (. .)),)\n"]}]},{"cell_type":"markdown","source":["Note: The constituent labels can be found [here](http://surdeanu.cs.arizona.edu/mihai/teaching/ista555-fall13/readings/PennTreebankConstituents.html)."],"metadata":{"id":"gAIH9cBqXRSu"}},{"cell_type":"markdown","source":["I'm now going to calculate the frequency of category bigrams. I.e., if a constituent of a given type immediately contains another of a given type, then that's a bigram.\n","\n","For example, an NP like *the dog*, which consists of a determine *the* and an noun *dog* would have bigrams (NP, Det) and (NP, N).\n","\n","To calculate category bigrams, I'm going to use a couple of functions:"],"metadata":{"id":"75A-wmZAXVyP"}},{"cell_type":"code","source":["def get_treelet_bigrams(treelet):\n","  cat = treelet.label\n","  daughters = treelet.children\n","  if cat in CFG_bigrams:\n","    for daughter in daughters:\n","      subcat = daughter.label\n","      if subcat in CFG_bigrams[cat]:\n","        CFG_bigrams[cat][subcat] += 1\n","      else:\n","        CFG_bigrams[cat][subcat] = 1\n","  else:\n","    CFG_bigrams[cat] = {}\n","    for daughter in daughters:\n","      subcat = daughter.label\n","      CFG_bigrams[cat][subcat] = 1"],"metadata":{"id":"eX0w5hXMKpVJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_CFG_bigrams(tree):\n","  subtrees = [tree]\n","  while not subtrees == []:\n","    daughters = []\n","    for subtree in subtrees:\n","      if type(subtree) == stanza.models.constituency.parse_tree.Tree:\n","        get_treelet_bigrams(subtree)\n","        daughters = daughters + list(subtree.children)\n","    subtrees = daughters"],"metadata":{"id":"K5un0V05OzR-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can use these to create a dictionary keeping track of the category bigrams in a set of sentences, like this:"],"metadata":{"id":"CKgud23nX5pz"}},{"cell_type":"code","source":["CFG_bigrams = {}\n","for sentence in doc.sentences:\n","  tree = sentence.constituency.children[0]\n","  get_CFG_bigrams(tree)\n","CFG_bigrams"],"metadata":{"id":"tTXabIv3NBBH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is a very simple example.\n","\n","What's more useful is to do this with a large set of sentences. We can use the Brown Corpus, for instance. I'm just going to use the first 100 sentences in the corpus, since sentences take a while to process."],"metadata":{"id":"OsfACKsZSvX5"}},{"cell_type":"code","source":["brownstr = ' '.join([' '.join(s) for s in brown.sents()[:100]])\n","doc = nlp(brownstr)"],"metadata":{"id":"6rq2pRlkNh4Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And now I'll get CFG bigrams."],"metadata":{"id":"8FWZrBEFLe4A"}},{"cell_type":"code","source":["CFG_bigrams = {}\n","for sentence in doc.sentences:\n","  tree = sentence.constituency.children[0]\n","  get_CFG_bigrams(tree)\n","CFG_bigrams"],"metadata":{"id":"chlZK6hCUuaW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Most of these are lexical items. I'll drop those."],"metadata":{"id":"xYiM_MFiVBnC"}},{"cell_type":"code","source":["old = CFG_bigrams.copy()\n","for key in old:\n","  if CFG_bigrams[key] == {}:\n","    CFG_bigrams.pop(key)\n","CFG_bigrams"],"metadata":{"id":"A4Kn3X4mVD3h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The results are a list of observed frequencies for each rule. With a large enough corpus, we could get a good measure of the probability of any given tree branch. We can then calculate the probability of a full tree by factoring in the probabilities of all its branches.\n","\n","In theory, we can use the probability of trees to weigh an autocorrection or speech recognition system.\n","\n"],"metadata":{"id":"-A4lEfl2KkVa"}}]}